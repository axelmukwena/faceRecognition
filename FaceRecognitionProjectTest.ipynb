{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FaceRecognitionProjectTest.ipynb","provenance":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"cell_type":"code","metadata":{"id":"w2kd9-i2S0ao","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"b23ca2dc-5e4a-4820-bbe2-e54d5ffe7f36","executionInfo":{"status":"ok","timestamp":1574513185612,"user_tz":-480,"elapsed":29423,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6n0zga2qXChIUfRXf-rtCeDfdXqNzET_HKUmOXPw=s64","userId":"13593993050630757383"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lxg_XNXuTNtr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"a3857742-3109-4c32-a0e8-c561c0519371","executionInfo":{"status":"ok","timestamp":1574513210903,"user_tz":-480,"elapsed":3613,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6n0zga2qXChIUfRXf-rtCeDfdXqNzET_HKUmOXPw=s64","userId":"13593993050630757383"}}},"source":["import os\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks/Face_Recognition\")\n","!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["aaaa\t\t\t      ORL_face_dataset_probe.txt\n","compute_recognition_rate.py   ORL_face_dataset_test_galary.txt\n","FaceRecognitionProject.ipynb  ORL_face_dataset_test_probe.txt\n","face_test.py\t\t      ORL_face_dataset_test.txt\n","face_train.py\t\t      ORL_face_dataset_train_random.txt\n","model_cache\t\t      ORL_face_dataset_train.txt\n","ORL_face_dataset\t      ORL_face_dataset_validation.txt\n","ORL_face_dataset_galary.txt   Report_template.docx\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xjtWSp5KTbJf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"5eadf251-ee50-4c9b-e5eb-b6508ae41b8f","executionInfo":{"status":"ok","timestamp":1574513345029,"user_tz":-480,"elapsed":82749,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA6n0zga2qXChIUfRXf-rtCeDfdXqNzET_HKUmOXPw=s64","userId":"13593993050630757383"}}},"source":["\n","# coding: utf-8\n","\n","# ## Parameters\n","\n","# In[1]:\n","\n","#LAMBDA = 0.0\n","LAMBDA = 0.0001\n","CENTER_LOSS_ALPHA = 0.0\n","NUM_CLASSES = 30\n","checkpoint_dir = \"./model_cache/\"\n","\n","# ## Import modules\n","\n","# In[2]:\n","\n","import os\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","import tflearn\n","#from batch_loader import BatchLoader\n","import numpy.random as nr\n","from random import shuffle\n","\n","\"\"\"\n","Batch Loader by Donny You\n","\"\"\"\n","\n","class BatchLoader(object):\n","\n","    def __init__(self, file_path, batch_size):\n","        self.batch_size = batch_size\n","        self.labels, self.im_list = self.image_dir_processor(file_path)\n","        self.idx = 0\n","        self.data_num = len(self.labels)\n","        self.rnd_list = np.arange(self.data_num)\n","        #shuffle(self.rnd_list)\n","\n","    def next_batch(self):\n","        batch_images = []\n","        batch_labels = []\n","\n","        for i in xrange (self.batch_size):\n","            if self.idx != self.data_num:\n","                cur_idx = self.rnd_list[self.idx]\n","                im_path = self.im_list[cur_idx]\n","                image = cv2.imread(im_path)\n","                #print image.shape\n","                #image = cv2.resize(image, (100, 100), interpolation=cv2.INTER_CUBIC)\n","                batch_images.append(image)\n","                batch_labels.append(self.labels[cur_idx])\n","\n","                self.idx +=1\n","            else:\n","                self.idx = 0\n","                shuffle(self.rnd_list)\n","\n","        batch_images = np.array(batch_images).astype(np.float32)\n","        batch_labels = np.array(batch_labels).astype(np.float32)\n","        return batch_images, batch_labels, im_path\n","        \n","    def image_dir_processor(self, file_path):\n","        labels = []\n","        im_path_list = []\n","        if not os.path.exists(file_path):\n","            print (\"File %s not exists.\" % file_path)\n","            exit()\n","\n","        with open(file_path, \"r\") as fr:\n","            for line in fr.readlines():\n","                terms = line.rstrip().split()\n","                label = int(terms[1])\n","                im_path_list.append(terms[0])\n","                labels.append(label)\n","\n","        return labels, im_path_list\n","\n","\n","\n","slim = tf.contrib.slim\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","\n","train_batch_loader = BatchLoader(\"ORL_face_dataset_train.txt\", 50)\n","#test_batch_loader = BatchLoader(\"ORL_face_dataset_validation.txt\", 30)\n","test_batch_loader = BatchLoader(\"ORL_face_dataset_test.txt\", 1)\n","#test_batch_loader = BatchLoader(\"ORL_face_dataset_test_probe.txt\", 1)\n","# ## Construct network\n","\n","# In[3]:\n","\n","with tf.name_scope('input'):\n","    input_images = tf.placeholder(tf.float32, shape=(None,112,92,3), name='input_images')\n","    labels = tf.placeholder(tf.int64, shape=(None), name='labels')\n","    \n","global_step = tf.Variable(0, trainable=False, name='global_step')\n","\n","\n","# In[4]:\n","\n","def get_center_loss(features, labels, alpha, num_classes):\n","    \"\"\"获取center loss及center的更新op\n","    \n","    Arguments:\n","        features: Tensor,表征样本特征,一般使用某个fc层的输出,shape应该为[batch_size, feature_length].\n","        labels: Tensor,表征样本label,非one-hot编码,shape应为[batch_size].\n","        alpha: 0-1之间的数字,控制样本类别中心的学习率,细节参考原文.\n","        num_classes: 整数,表明总共有多少个类别,网络分类输出有多少个神经元这里就取多少.\n","    \n","    Return：\n","        loss: Tensor,可与softmax loss相加作为总的loss进行优化.\n","        centers: Tensor,存储样本中心值的Tensor，仅查看样本中心存储的具体数值时有用.\n","        centers_update_op: op,用于更新样本中心的op，在训练时需要同时运行该op，否则样本中心不会更新\n","    \"\"\"\n","    # 获取特征的维数，例如256维\n","    # print features.get_shape()\n","    len_features = features.get_shape()[1]\n","    # 建立一个Variable,shape为[num_classes, len_features]，用于存储整个网络的样本中心，\n","    # 设置trainable=False是因为样本中心不是由梯度进行更新的\n","    centers = tf.get_variable('centers', [num_classes, len_features], dtype=tf.float32,\n","        initializer=tf.constant_initializer(0), trainable=False)\n","    # 将label展开为一维的，输入如果已经是一维的，则该动作其实无必要\n","    labels = tf.reshape(labels, [-1])\n","    \n","    # 根据样本label,获取mini-batch中每一个样本对应的中心值\n","    centers_batch = tf.gather(centers, labels)\n","    # 计算loss\n","    loss = tf.nn.l2_loss(features - centers_batch)\n","    \n","    # 当前mini-batch的特征值与它们对应的中心值之间的差\n","    diff = centers_batch - features\n","    \n","    # 获取mini-batch中同一类别样本出现的次数,了解原理请参考原文公式(4)\n","    unique_label, unique_idx, unique_count = tf.unique_with_counts(labels)\n","    appear_times = tf.gather(unique_count, unique_idx)\n","    appear_times = tf.reshape(appear_times, [-1, 1])\n","    \n","    diff = diff / tf.cast((1 + appear_times), tf.float32)\n","    diff = alpha * diff\n","    \n","    centers_update_op = tf.scatter_sub(centers, labels, diff)\n","    \n","    return loss, centers, centers_update_op\n","\n","\n","# In[5]:\n","\n","\n","\n","def inference(input_images):\n","    with slim.arg_scope([slim.conv2d], \n","                         activation_fn=tflearn.prelu, stride=1, padding='SAME',\n","                         weights_initializer=tf.truncated_normal_initializer(stddev=0.01)):\n","                         # weights_initializer=tf.contrib.layers.xavier_initializer()):\n","        x = slim.conv2d(input_images, 32, [3, 3],\n","                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n","                        padding='VALID', scope='conv1a')\n","\n","        x = slim.conv2d(x, 64, [3, 3], \n","                        weights_initializer=tf.contrib.layers.xavier_initializer(), \n","                        padding='VALID', scope='conv1b')\n","\n","        pool1b = slim.max_pool2d(x, [2, 2], stride=2, padding='VALID', scope='pool1b')\n","\n","        conv2_1 = slim.conv2d(pool1b, 64, [3, 3], scope='conv2_1')\n","        conv2_2 = slim.conv2d(conv2_1, 64, [3, 3], scope='conv2_2')\n","        res2_2 = pool1b + conv2_2\n","        conv2 = slim.conv2d(res2_2, 128, [3, 3],\n","                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n","                        padding='VALID', scope='conv2')\n","\n","        pool2 = slim.max_pool2d(conv2, [2, 2], stride=2, padding='VALID', scope='pool2')\n","        conv3_1 = slim.conv2d(pool2, 128, [3, 3], scope='conv3_1')\n","        conv3_2 = slim.conv2d(conv3_1, 128, [3, 3], scope='conv3_2')\n","        res3_2 = pool2 + conv3_2\n","\n","        conv3_3 = slim.conv2d(res3_2, 128, [3, 3], scope='conv3_3')\n","        conv3_4 = slim.conv2d(conv3_3, 128, [3, 3], scope='conv3_4')\n","        res3_4 = res3_2 + conv3_4\n","\n","        conv3 = slim.conv2d(res3_4, 256, [3, 3],\n","                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n","                        padding='VALID', scope='conv3')\n","        pool3 = slim.max_pool2d(conv3, [2, 2], stride=2, padding='VALID', scope='pool3')\n","        conv4_1 = slim.conv2d(pool3, 256, [3, 3], scope='conv4_1')\n","        conv4_2 = slim.conv2d(conv4_1, 256, [3, 3], scope='conv4_2')\n","        res4_2 = pool3 + conv4_2\n","\n","        conv4_3 = slim.conv2d(res4_2, 256, [3, 3], scope='conv4_3')\n","        conv4_4 = slim.conv2d(conv4_3, 256, [3, 3], scope='conv4_4')\n","        res4_4 = res4_2 + conv4_4\n","\n","        conv4_5 = slim.conv2d(res4_4, 256, [3, 3], scope='conv4_5')\n","        conv4_6 = slim.conv2d(conv4_5, 256, [3, 3], scope='conv4_6')\n","        res4_6 = res4_4 + conv4_6\n","\n","        conv4_7 = slim.conv2d(res4_6, 256, [3, 3], scope='conv4_7')\n","        conv4_8 = slim.conv2d(conv4_7, 256, [3, 3], scope='conv4_8')\n","        res4_8 = res4_6 + conv4_8\n","\n","        conv4_9 = slim.conv2d(res4_8, 256, [3, 3], scope='conv4_9')\n","        conv4_10 = slim.conv2d(conv4_9, 256, [3, 3], scope='conv4_10')\n","        res4_10 = res4_8 + conv4_10\n","\n","        conv4 = slim.conv2d(res4_10, 512, [3, 3],\n","                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n","                        padding='VALID', scope='conv4')\n","        pool4 = slim.max_pool2d(conv4, [2, 2], stride=2, padding='VALID', scope='pool4')\n","        \n","        conv5_1 = slim.conv2d(pool4, 512, [3, 3], scope='conv5_1')\n","        conv5_2 = slim.conv2d(conv5_1, 512, [3, 3], scope='conv5_2')\n","        res5_2 = pool4 + conv5_2\n","\n","        conv5_3 = slim.conv2d(res5_2, 512, [3, 3], scope='conv5_3')\n","        conv5_4 = slim.conv2d(conv5_3, 512, [3, 3], scope='conv5_4')\n","        res5_4 = res5_2 + conv5_4\n","\n","        conv5_5 = slim.conv2d(res5_4, 512, [3, 3], scope='conv5_5')\n","        conv5_6 = slim.conv2d(conv5_5, 512, [3, 3], scope='conv5_6')\n","        res5_6 = res5_4 + conv5_6\n","        res5_6 = slim.flatten(res5_6, scope='flatten')\n","        feature = slim.fully_connected(res5_6, num_outputs=512, activation_fn=None, \n","                            weights_initializer=tf.contrib.layers.xavier_initializer(), scope='fc1')\n","\n","        x = slim.fully_connected(feature, num_outputs=NUM_CLASSES, activation_fn=None, scope='fc2')\n","    \n","    return x, feature\n","\n","\n","# In[6]:\n","\n","def build_network(input_images, labels, ratio=0.5):\n","    logits, features = inference(input_images)\n","    \n","    with tf.name_scope('loss'):\n","        with tf.name_scope('center_loss'):\n","            center_loss, centers, centers_update_op = get_center_loss(features, labels, CENTER_LOSS_ALPHA, NUM_CLASSES)\n","        with tf.name_scope('softmax_loss'):\n","            softmax_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n","        with tf.name_scope('total_loss'):\n","            total_loss = softmax_loss + ratio * center_loss\n","    \n","    with tf.name_scope('acc'):\n","        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(logits, 1), labels), tf.float32))\n","    '''\n","    with tf.name_scope('loss/'):\n","        tf.summary.scalar('CenterLoss', center_loss)\n","        tf.summary.scalar('SoftmaxLoss', softmax_loss)\n","        tf.summary.scalar('TotalLoss', total_loss)\n","    '''\n","    return logits, features, total_loss, accuracy, centers_update_op, center_loss, softmax_loss\n","\n","\n","# In[7]:\n","\n","logits, features, total_loss, accuracy, centers_update_op, center_loss, softmax_loss = build_network(input_images, labels, ratio=LAMBDA)\n","\n","\n","# ## Prepare data\n","\n","# In[8]:\n","\n","# mnist = input_data.read_data_sets('/tmp/mnist', reshape=False)\n","\n","\n","# ## Optimizer\n","\n","# In[9]:\n","\n","optimizer = tf.train.AdamOptimizer(0.001)\n","\n","\n","# In[10]:\n","\n","with tf.control_dependencies([centers_update_op]):\n","    train_op = optimizer.minimize(total_loss, global_step=global_step)\n","\n","\n","# ## Session and Summary\n","\n","# In[11]:\n","\n","summary_op = tf.summary.merge_all()\n","\n","\n","# In[12]:\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","writer = tf.summary.FileWriter('/tmp/mnist_log', sess.graph)\n","\n","\n","# ## Train\n","# In[14]:\n","\n","#sess = tf.Session()\n","#init_op = tf.global_variables_initializer()\n","#sess.run(init_op)\n","saver = tf.train.Saver(max_to_keep=0)\n","saver.restore(sess,'model_cache/model.ckpt-1200')\n","#image = cv2.imread(\"test_file/9.BMP\")\n","#print image\n","for i in range(100):\n","    batch_images, batch_labels, im_path = test_batch_loader.next_batch()\n","    vali_image = (batch_images - 127.5) * 0.0078125\n","    vali_features = sess.run(\n","        features,\n","        feed_dict={\n","            input_images: vali_image,\n","            labels: batch_labels\n","        })\n","    #print vali_features.shape\n","    #print vali_features\n","    #print im_path\n","    words=im_path.split('/')\n","    #index_face=words[].split('/')\n","    feature_save_folder='face_feature/'+words[1]\n","    feature_save_name='face_feature/'+words[1]+'/'+words[2].split('.')[0]+'.npy'\n","    print (feature_save_name)\n","    if not os.path.exists(feature_save_folder):\n","        os.makedirs(feature_save_folder)\n","    np.save(feature_save_name, vali_features)\n","\n","#c = np.load(feature_save_name)\n","#print c\n","\n","sess.close()\n","\n"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W1123 12:47:47.797197 140673144321920 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n","\n","W1123 12:47:47.798804 140673144321920 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","W1123 12:47:47.812715 140673144321920 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","W1123 12:47:47.823654 140673144321920 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","W1123 12:47:47.838380 140673144321920 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n","\n","W1123 12:47:47.839669 140673144321920 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","W1123 12:47:47.848123 140673144321920 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","W1123 12:47:48.577949 140673144321920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","W1123 12:47:48.614078 140673144321920 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/tflearn/activations.py:225: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","W1123 12:47:49.111824 140673144321920 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n"],"name":"stderr"},{"output_type":"stream","text":["face_feature/s31/1.npy\n","face_feature/s31/2.npy\n","face_feature/s31/3.npy\n","face_feature/s31/4.npy\n","face_feature/s31/5.npy\n","face_feature/s32/1.npy\n","face_feature/s32/2.npy\n","face_feature/s32/3.npy\n","face_feature/s32/4.npy\n","face_feature/s32/5.npy\n","face_feature/s33/1.npy\n","face_feature/s33/2.npy\n","face_feature/s33/3.npy\n","face_feature/s33/4.npy\n","face_feature/s33/5.npy\n","face_feature/s34/1.npy\n","face_feature/s34/2.npy\n","face_feature/s34/3.npy\n","face_feature/s34/4.npy\n","face_feature/s34/5.npy\n","face_feature/s35/1.npy\n","face_feature/s35/2.npy\n","face_feature/s35/3.npy\n","face_feature/s35/4.npy\n","face_feature/s35/5.npy\n","face_feature/s36/1.npy\n","face_feature/s36/2.npy\n","face_feature/s36/3.npy\n","face_feature/s36/4.npy\n","face_feature/s36/5.npy\n","face_feature/s37/1.npy\n","face_feature/s37/2.npy\n","face_feature/s37/3.npy\n","face_feature/s37/4.npy\n","face_feature/s37/5.npy\n","face_feature/s38/1.npy\n","face_feature/s38/2.npy\n","face_feature/s38/3.npy\n","face_feature/s38/4.npy\n","face_feature/s38/5.npy\n","face_feature/s39/1.npy\n","face_feature/s39/2.npy\n","face_feature/s39/3.npy\n","face_feature/s39/4.npy\n","face_feature/s39/5.npy\n","face_feature/s40/1.npy\n","face_feature/s40/2.npy\n","face_feature/s40/3.npy\n","face_feature/s40/4.npy\n","face_feature/s40/5.npy\n","face_feature/s31/6.npy\n","face_feature/s31/7.npy\n","face_feature/s31/8.npy\n","face_feature/s31/9.npy\n","face_feature/s31/10.npy\n","face_feature/s32/6.npy\n","face_feature/s32/7.npy\n","face_feature/s32/8.npy\n","face_feature/s32/9.npy\n","face_feature/s32/10.npy\n","face_feature/s33/6.npy\n","face_feature/s33/7.npy\n","face_feature/s33/8.npy\n","face_feature/s33/9.npy\n","face_feature/s33/10.npy\n","face_feature/s34/6.npy\n","face_feature/s34/7.npy\n","face_feature/s34/8.npy\n","face_feature/s34/9.npy\n","face_feature/s34/10.npy\n","face_feature/s35/6.npy\n","face_feature/s35/7.npy\n","face_feature/s35/8.npy\n","face_feature/s35/9.npy\n","face_feature/s35/10.npy\n","face_feature/s36/6.npy\n","face_feature/s36/7.npy\n","face_feature/s36/8.npy\n","face_feature/s36/9.npy\n","face_feature/s36/10.npy\n","face_feature/s37/6.npy\n","face_feature/s37/7.npy\n","face_feature/s37/8.npy\n","face_feature/s37/9.npy\n","face_feature/s37/10.npy\n","face_feature/s38/6.npy\n","face_feature/s38/7.npy\n","face_feature/s38/8.npy\n","face_feature/s38/9.npy\n","face_feature/s38/10.npy\n","face_feature/s39/6.npy\n","face_feature/s39/7.npy\n","face_feature/s39/8.npy\n","face_feature/s39/9.npy\n","face_feature/s39/10.npy\n","face_feature/s40/6.npy\n","face_feature/s40/7.npy\n","face_feature/s40/8.npy\n","face_feature/s40/9.npy\n","face_feature/s40/10.npy\n"],"name":"stdout"}]}]}